# üìö Sistema Completo de Credit Risk Analysis

## üéØ Resumen Ejecutivo

Este sistema eval√∫a el riesgo crediticio de clientes usando Machine Learning. El flujo completo es:

1. **Entrenamiento**: Se procesa el dataset, se entrenan modelos y se guarda el mejor.
2. **Predicci√≥n**: El usuario completa un formulario en la UI ‚Üí API procesa los datos ‚Üí Modelo predice ‚Üí Se muestra el resultado.

---

## üîÑ Flujo Completo del Sistema

### 1Ô∏è‚É£ **Fase de Entrenamiento** (`src/train_model.py`)

```
Dataset Original (50,000 filas √ó 53 columnas)
    ‚Üì
PreprocessingPipeline.fit_transform()
    ‚Üì Transformaci√≥n autom√°tica:
      - Limpieza (remueve constantes y columnas alta cardinalidad+missing, normaliza Y/N)
      - Feature Engineering (crea 17 nuevas features)
      - Manejo de missing values (imputaci√≥n + 6 indicadores)
      - Encoding (OneHot para baja cardinalidad, Ordinal para alta)
      - Scaling (MinMaxScaler 0-1)
    ‚Üì
Dataset Procesado (50,000 filas √ó 286 features num√©ricas)
    ‚Üì
Entrenamiento de 3 modelos:
  - Logistic Regression (class_weight='balanced')
  - Random Forest (class_weight='balanced')
  - Gradient Boosting (sample_weight='balanced')
    ‚Üì
Evaluaci√≥n en conjunto de validaci√≥n
    ‚Üì
C√°lculo de threshold √≥ptimo (Youden's J statistic)
    ‚Üì
Guardado del mejor modelo:
  ‚úì models/production/model.joblib (modelo)
  ‚úì models/preprocessor/preprocessor.joblib (pipeline)
  ‚úì models/production/optimal_threshold.txt (threshold √≥ptimo)
  ‚úì models/production/metrics.txt (m√©tricas de rendimiento)
```

**Nota importante:** El preprocessing NO guarda archivos CSV procesados. En su lugar, guarda el **pipeline entrenado** (`preprocessor.joblib`) que puede reutilizarse para cualquier dato nuevo.

---

### 2Ô∏è‚É£ **Fase de Predicci√≥n** (API + UI)

```
Usuario completa formulario en UI (Streamlit)
    ‚Üì Solo proporciona campos esenciales (otros son opcionales)
UI construye request JSON con features b√°sicas
    ‚Üì
UI env√≠a POST request a API (FastAPI)
    ‚Üì
API recibe request simplificado
    ‚Üì
Feature Mapper completa features faltantes:
  - Agrega las 9 columnas constantes (valores por defecto)
  - Completa campos opcionales con valores por defecto o None
  - Ordena columnas en el orden correcto del dataset original
    ‚Üì
API crea DataFrame con todas las 53 columnas originales
    ‚Üì
PreprocessingPipeline.transform() (usa pipeline guardado)
  - Aplica TODAS las transformaciones guardadas
  - Mismo procesamiento que durante entrenamiento
  - Resultado: 286 features num√©ricas finales
    ‚Üì
Modelo.predict_proba() ‚Üí Obtiene probabilidad de default (0-1)
    ‚Üì
API compara probabilidad con optimal_threshold (0.5059):
  - Si probabilidad ‚â• 0.5059 ‚Üí RECHAZADO
  - Si probabilidad < 0.5059 ‚Üí APROBADO
    ‚Üì
API retorna respuesta JSON:
  {
    "prediction": "approved" o "rejected",
    "probability": 0.XX,
    "confidence": "high/medium/low"
  }
    ‚Üì
UI muestra resultado al usuario con explicaci√≥n
```

---

## üîß Componentes del Sistema

### **PreprocessingPipeline** (`src/preprocessing.py`)

Pipeline reutilizable que transforma datos raw en formato que el modelo entiende. Consta de **6 pasos secuenciales**:

1. **Limpieza Inicial**

   - Remueve `ID_CLIENT`
   - Convierte flags Y/N ‚Üí 0/1
   - Identifica y remueve **9 columnas constantes**

2. **Manejo de Outliers**

   - No se aplica Winsorization
   - Basado en el EDA, el porcentaje de outliers es bajo (~2% m√°ximo)
   - Los valores extremos son informativos para credit risk

3. **Feature Engineering**

   - Crea **17 nuevas features**:
     - Ratios financieros (ingresos/activos, ingresos por dependiente, etc.)
     - Scores de estabilidad (a√±os en residencia/trabajo)
     - Conteos (tarjetas, m√©todos de contacto)
     - Comparaciones geogr√°ficas (estado residencia vs nacimiento, mismo ZIP, etc.)
     - Features de cuentas bancarias
     - Edad al cuadrado (relaciones no lineales)
   - **Nota:** Features de documentos fueron removidas (usaban columnas constantes)

4. **Manejo de Missing Values**

   - Crea **6 indicadores binarios** para missing importantes
   - Imputa: moda para categ√≥ricas, mediana para num√©ricas
   - **Resultado:** ~62 ‚Üí ~68 columnas

5. **Encoding**

   - **Binarias (2 valores):** OrdinalEncoder
   - **Baja cardinalidad (‚â§20 categor√≠as):** OneHotEncoder
   - **Alta cardinalidad (>20 categor√≠as):** OrdinalEncoder
   - **Resultado:** 286 features num√©ricas finales

6. **Scaling**
   - MinMaxScaler (normaliza todas las features a rango 0-1)

**Por qu√© guardamos el pipeline y no los datos procesados:**

- ‚úÖ Reutilizable para nuevos datos
- ‚úÖ Menos espacio (solo guarda transformadores, no datos)
- ‚úÖ Consistencia garantizada (mismo preprocessing siempre)

---

### **Feature Mapper** (`src/api/feature_mapper.py`)

Convierte el input simplificado de la UI al formato completo que requiere el modelo:

- Completa las **9 columnas constantes** (que se eliminan despu√©s pero deben estar presentes)
- Rellena campos opcionales con valores por defecto o `None`
- Ordena las columnas en el orden correcto del dataset original
- Garantiza que el DataFrame tenga exactamente 53 columnas antes del preprocessing

---

### **Modelo** (`models/production/model.joblib`)

**Modelo seleccionado:** Gradient Boosting Classifier

**M√©tricas actuales:**

- **ROC-AUC:** 0.64 (capacidad de distinguir entre clases)
- **F1:** 0.44 (balance entre precisi√≥n y recall)
- **Precision:** 0.35 (cuando predice "riesgoso", ¬øcu√°ntas veces tiene raz√≥n?)
- **Recall:** 0.58 (¬øqu√© % de riesgosos detecta?)

**Threshold √≥ptimo:** 0.5059 (calculado din√°micamente usando Youden's J statistic)

---

## üíæ Formato de Archivos: Joblib

**¬øPor qu√© usamos `.joblib` en vez de `.pkl`?**

- ‚úÖ Especializado para modelos sklearn y arrays NumPy
- ‚úÖ M√°s eficiente con objetos grandes
- ‚úÖ Usado por defecto en sklearn
- ‚úÖ Mejor compatibilidad entre versiones

**Archivos guardados:**

- `model.joblib`: Modelo entrenado (Gradient Boosting)
- `preprocessor.joblib`: Pipeline de preprocessing completo
- Ambos archivos son portables y pueden compartirse con otros desarrolladores

---

## üìä Estado Actual y Mejoras Aplicadas

### **Rendimiento del Modelo**

El modelo mejor√≥ significativamente con las optimizaciones aplicadas:

| M√©trica     | Antes | Ahora | Mejora         |
| ----------- | ----- | ----- | -------------- |
| **Recall**  | 0.08  | 0.58  | **7x mejor**   |
| **F1**      | 0.13  | 0.44  | **3.4x mejor** |
| **ROC-AUC** | 0.63  | 0.64  | Estable        |

### **Optimizaciones Implementadas**

1. ‚úÖ **Threshold √≥ptimo calculado din√°micamente** (se calcula autom√°ticamente para cada modelo)
2. ‚úÖ **Balanceo de clases** en todos los modelos
3. ‚úÖ **17 nuevas features** de feature engineering
4. ‚úÖ **6 indicadores de missing** para capturar informaci√≥n faltante
5. ‚úÖ **Hiperpar√°metros optimizados** (m√°s √°rboles, profundidad controlada)
6. ‚úÖ **UI mejorada** con selectboxes descriptivos y opciones realistas
7. ‚úÖ **Campos opcionales manejan `None`** correctamente (no `0`)

---

## üöÄ Gu√≠a de Inicio R√°pido

### **Para alguien que descarga el proyecto por primera vez**

Esta gu√≠a te ayudar√° a configurar el dataset, levantar Docker, entrenar el modelo y probarlo.

---

### **Prerequisitos**

1. **Docker y Docker Compose** instalados y funcionando
2. **Datos del dataset** listos para colocar en `data/raw/`

---

### **Paso 1: Preparar el Dataset**

Aseg√∫rate de tener los archivos del dataset en la carpeta `data/raw/`:

```bash
data/raw/
  ‚îú‚îÄ‚îÄ PAKDD2010_Modeling_Data.txt
  ‚îî‚îÄ‚îÄ PAKDD2010_VariablesList.XLS
```

**Importante:** Los archivos deben estar en esta ubicaci√≥n antes de entrenar el modelo.

---

### **Paso 2: Levantar el Sistema con Docker**

Construye y levanta todos los servicios (UI, API y Model):

```bash
# Primera vez (construye las im√°genes)
docker-compose up --build

# Siguientes veces (m√°s r√°pido, usa im√°genes existentes)
docker-compose up
```

**¬øQu√© hace esto?**

- ‚úÖ Construye las im√°genes Docker para UI, API y Model
- ‚úÖ Levanta los 3 servicios en contenedores separados
- ‚úÖ Configura la red interna entre servicios
- ‚úÖ Monta los vol√∫menes necesarios (datos, modelos, etc.)

**Servicios disponibles:**

- üåê **UI:** http://localhost:8501 (Streamlit)
- üîå **API:** http://localhost:8000 (FastAPI)
- üìä **API Docs:** http://localhost:8000/docs (Swagger UI)

**Nota:** La primera vez puede tardar varios minutos en descargar e instalar dependencias.

---

### **Paso 3: Entrenar el Modelo**

Con Docker levantado, ejecuta el entrenamiento dentro del contenedor de la API (que tiene acceso a todos los datos):

```bash
# Ejecutar entrenamiento dentro del contenedor API
docker-compose exec api python -m src.train_model
```

**O si prefieres entrenar localmente** (con Python instalado en tu m√°quina):

```bash
# Instalar dependencias localmente (solo si no usas Docker)
pip install -r requirements.txt

# Ejecutar entrenamiento
python -m src.train_model
```

**¬øQu√© hace este comando?**

1. ‚úÖ Carga el dataset desde `data/raw/`
2. ‚úÖ Ejecuta el preprocessing completo (6 pasos)
3. ‚úÖ Entrena 3 modelos (Logistic Regression, Random Forest, Gradient Boosting)
4. ‚úÖ Eval√∫a y selecciona el mejor modelo
5. ‚úÖ Calcula el threshold √≥ptimo
6. ‚úÖ Guarda todo en:
   - `models/production/model.joblib`
   - `models/preprocessor/preprocessor.joblib` (nueva ubicaci√≥n)
   - `models/production/optimal_threshold.txt`
   - `models/production/metrics.txt`

**Tiempo estimado:** 1-3 minutos (depende del hardware)

**Al finalizar ver√°s:**

- M√©tricas de cada modelo
- Modelo seleccionado como mejor
- Confirmaci√≥n de archivos guardados

**Importante:** Despu√©s de entrenar, reinicia el servicio API para que cargue el nuevo modelo:

```bash
docker-compose restart api
```

---

### **Paso 4: Probar el Sistema con la UI**

#### **Opci√≥n A: Usar la UI (Recomendado)**

1. Abre tu navegador en: **http://localhost:8501**
2. Completa el formulario con los datos de un cliente
3. Haz clic en "Evaluar Riesgo Crediticio"
4. Ver√°s el resultado: **APROBADO** o **RECHAZADO** con la probabilidad

#### **Opci√≥n B: Usar la API directamente**

```bash
# Ejemplo de request usando curl
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "PAYMENT_DAY": 15,
    "APPLICATION_SUBMISSION_TYPE": "Web",
    "SEX": "M",
    "AGE": 35,
    "QUANT_DEPENDANTS": 1,
    "PERSONAL_MONTHLY_INCOME": 5000.0,
    "FLAG_RESIDENCIAL_PHONE": "Y",
    "COMPANY": "Y",
    "FLAG_PROFESSIONAL_PHONE": "Y"
  }'
```

**Respuesta esperada:**

```json
{
  "prediction": "approved",
  "probability": 0.4231,
  "confidence": "medium"
}
```

#### **Opci√≥n C: Usar la documentaci√≥n interactiva**

1. Abre: **http://localhost:8000/docs**
2. Expande el endpoint `/predict`
3. Haz clic en "Try it out"
4. Completa el JSON de ejemplo
5. Haz clic en "Execute"
6. Ver√°s la respuesta directamente en el navegador

---

### **Comandos √ötiles**

```bash
# Ver logs de todos los servicios
docker-compose logs -f

# Ver logs de un servicio espec√≠fico
docker-compose logs -f api
docker-compose logs -f ui

# Detener servicios
docker-compose down

# Detener y eliminar vol√∫menes (limpia todo)
docker-compose down -v

# Reconstruir un servicio espec√≠fico
docker-compose build --no-cache api
docker-compose up api
```

---

### **Verificar que Todo Funciona**

1. ‚úÖ **API health check:**

   ```bash
   curl http://localhost:8000/health
   ```

   Debe retornar: `{"status":"ok","model_loaded":true,"preprocessor_loaded":true}`

2. ‚úÖ **API model info:**

   ```bash
   curl http://localhost:8000/model_info
   ```

   Debe mostrar informaci√≥n del modelo cargado

3. ‚úÖ **UI carga correctamente:** http://localhost:8501 muestra el formulario

---

### **Solucionar Problemas Comunes**

**Problema:** `FileNotFoundError: data/raw/PAKDD2010_Modeling_Data.txt`

- **Soluci√≥n:** Verifica que los archivos del dataset est√©n en `data/raw/`

**Problema:** `Model or preprocessor not loaded`

- **Soluci√≥n:** Aseg√∫rate de haber ejecutado `python -m src.train_model` primero

**Problema:** API retorna error 500

- **Soluci√≥n:** Revisa los logs: `docker-compose logs api`
- Verifica que `scikit-learn==1.6.1` est√© instalado (versi√≥n debe coincidir)

**Problema:** UI muestra error al cargar `ui_options.json`

- **Soluci√≥n:** Verifica que el archivo `src/ui/ui_options.json` exista. Si falta, la UI funcionar√° igual pero con opciones limitadas.

---

## üìù Resumen del Flujo Completo

```
1. Preparar dataset ‚Üí data/raw/
2. Levantar Docker ‚Üí docker-compose up --build
3. Entrenar modelo ‚Üí docker-compose exec api python -m src.train_model
4. Probar sistema ‚Üí http://localhost:8501
```

¬°Listo! Ya tienes el sistema completo funcionando. üéâ
